{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deriving Term Frequency from CWB Weather Summary\n",
    "\n",
    "這篇筆記要示範自然語言處理（[Natural Language Processing](https://en.wikipedia.org/wiki/Natural_language_processing), NLP）中的基本工具：計算 [term frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)。我們想要從氣象局每天的[「天氣概況」](https://www.cwb.gov.tw/V8/C/W/index.html)裡，找出氣象局專家群常常提及的一些台灣及鄰近區域的天氣現象。\n",
    "\n",
    "以下為氣象局天氣概況的一則範例：\n",
    "\n",
    "***\n",
    "```\n",
    "07fW01136\n",
    "中央氣象局氣象報告\n",
    "１１０年３月１３日１１時０分發布\n",
    "３月１３日８時天氣概況：\n",
    "一、高氣壓１０２６百帕，在北緯３２度，東經１２０度，即在江蘇，向東緩慢移動。\n",
    "二、今、明（１３日、１４日）兩天東北季風影響，北部及東北部天氣較涼，其他地區早晚亦涼；臺灣東半部地區有局部短暫雨，其他地區及澎湖、金門、馬祖為多雲到晴，今日午後新竹以南山區亦有局部短暫陣雨，明日新竹以南山區亦有零星短暫雨；今、明兩天桃園至臺南、東南部（含蘭嶼、綠島）、恆春半島沿海空曠地區及澎湖、金門易有較強陣風；明日西半部地區易有局部霧或低雲影響能見度，請注意。\n",
    "三、海上強風特報：\n",
    "１、東北風偏強，臺灣海峽北部平均風力６至７級，最大陣風９級；臺灣北部海面、臺灣東南部海面及臺灣海峽南部平均風力可達６級，最大陣風８級，船隻請特別注意。今（１３）日臺灣東南部海面及臺灣海峽平均風力將稍減弱。明（１４日）下午起臺灣東南部海面平均風力將增強至６級，最大陣風８級，船隻請注意。\n",
    "２、東北風偏強，巴士海峽、廣東海面及南海平均風力可達６級，雷雨區最大陣風８至９級，船隻請注意。今（１３日）晚起中西沙島海面平均風力將增強至６到７級，最大陣風９級，船隻請特別注意。今（１３）日廣東海面平均風力將稍減弱。\n",
    "\n",
    "```\n",
    "***\n",
    "\n",
    "我們希望能從內容裡中找出像是「高氣壓」、「東北季風」、「局部霧」、「低雲」、以及「強風特報」、「颱風警報」這類指涉特定天氣系統或現象的詞彙，並透過分析長時間累積的文本資料，計算各個詞彙出現的頻率，來看看中央氣象局最常提到的天氣現象有哪些。\n",
    "\n",
    "\n",
    "\n",
    "## 工具程式碼\n",
    "\n",
    "這個工具的開發過程詳述在 [Deriving Term Frequency from CWB Weather Summary](https://github.com/tingsyo/data_science_practioner_notes/blob/master/notebook/NLP_term_frequency_from%20CWB_weather_summary.ipynb) 這篇筆記裡，以下我們直接看測試調整過後的結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取單一檔案\n",
    "def read_cwb_summary(furi):\n",
    "    import os, re\n",
    "    # Read file content\n",
    "    with open(furi, 'r', encoding='utf8') as f:\n",
    "        try:\n",
    "            text = f.readlines()\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(furi)\n",
    "    text = ''.join(text)\n",
    "    return(text)\n",
    "\n",
    "# 利用 jieba 分詞\n",
    "def calculate_tf_with_jieba(article, userdict, char_to_remove):\n",
    "    import jieba\n",
    "    jieba.load_userdict(userdict)    # Load user-dictionary\n",
    "    # Perform word segmentation\n",
    "    seg_list = jieba.cut(article, cut_all=True)\n",
    "    # Count word frequency\n",
    "    wc={}\n",
    "    for w in list(seg_list):\n",
    "        if not w in wc:\n",
    "            wc[w] = 1\n",
    "        else:\n",
    "            wc[w] += 1\n",
    "    # Remove keys containing character to remove\n",
    "    for term in char_to_remove:\n",
    "        for k in wc.copy().keys():\n",
    "            if term in k:\n",
    "                del wc[k]\n",
    "    # Done\n",
    "    return(wc)\n",
    "\n",
    "# Merge two dicts\n",
    "def merge_dictionary(dict1, dict2):\n",
    "    newdict = dict1.copy()\n",
    "    for k, v in dict2.items():\n",
    "        #print(k,v)\n",
    "        if k not in newdict.keys():\n",
    "            newdict[k]=v\n",
    "        else:\n",
    "            newdict[k]+=v\n",
    "    return(newdict)\n",
    "\n",
    "# \n",
    "def count_words(DATAPATH, SURFIX = '_1100.W01.dat',\n",
    "                userdict='../data/cwb/userdict.txt', \n",
    "                ignorelist='../data/cwb/ignore.txt',\n",
    "                min_count=1, min_length=1):\n",
    "    import os, re\n",
    "    import pandas as pd\n",
    "    import jieba\n",
    "    from collections import Counter\n",
    "    # Get files\n",
    "    fileinfo = []\n",
    "    for root, dirs, files in os.walk(DATAPATH):\n",
    "        for name in files:\n",
    "            if name.endswith(SURFIX):\n",
    "                date = name.replace(SURFIX, '')\n",
    "                furi = (os.path.join(root, name))\n",
    "                fileinfo.append({'date':date, 'uri': furi})\n",
    "    fileinfo = pd.DataFrame(fileinfo)\n",
    "    #print(fileinfo.head())\n",
    "    # Define characters to remove\n",
    "    char_to_remove = ['１', '２', '３', '４', '５', '６', '７', '８', '９', '０', \n",
    "                      '，', '。', '、', '(', ')', '（', '）', '；', '\\n', '\\u3000','07fW']\n",
    "    # Perform word segmentation and counts\n",
    "    data = read_cwb_summary(fileinfo['uri'].iloc[0])\n",
    "    wc = calculate_tf_with_jieba(data, userdict, char_to_remove)\n",
    "    for i in range(1,fileinfo.shape[0]):\n",
    "        data = read_cwb_summary(fileinfo['uri'].iloc[i])\n",
    "        tmp = calculate_tf_with_jieba(data, userdict, char_to_remove)\n",
    "        wc = merge_dictionary(wc, tmp)\n",
    "    # Load ignore-list\n",
    "    with open(ignorelist, 'r', encoding='utf8') as f:\n",
    "        to_ignore = f.read().split('\\n')\n",
    "    to_ignore.append('\\n')\n",
    "    to_ignore.append('\\u3000')\n",
    "    # Remove terms in ignore list\n",
    "    for term in to_ignore:\n",
    "        if term in wc.copy().keys():\n",
    "            del wc[term]\n",
    "    # Remove terms with length less than min_mlength\n",
    "    if min_length>1:\n",
    "        for k in wc.copy().keys():\n",
    "            if len(k)<min_length:\n",
    "                del wc[k]\n",
    "    if min_count>1:\n",
    "        for k in wc.copy().keys():\n",
    "            if wc[k]<min_count:\n",
    "                del wc[k]\n",
    "    # Sort by counts\n",
    "    wc = dict(sorted(wc.items(), key=lambda item: item[1], reverse=True))\n",
    "    # Done\n",
    "    return(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\tsyo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.557 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'最大陣風': 18827, '雷雨': 8242, '雷雨區': 6442, '東北風': 2846, '鋒面': 2812, '低氣壓': 1642, '雷陣雨': 1493, '東北季風': 1389, '大雨': 1256, '強陣風': 993, '熱帶性低氣壓': 924, '豪雨': 289, '華南雲雨區': 158, '寒流': 142, '太平': 120, '西南氣流': 108, '拉西': 68, '溫帶氣旋': 57, '降雨': 49, '威克': 36, '安全': 35, '注意安全': 31, '天寒': 30, '雅浦': 21, '哈隆': 20}\n"
     ]
    }
   ],
   "source": [
    "wc = count_words('D:/data/cwb_report', min_count=20, min_length=2)\n",
    "print(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "   year    雷雨   雷陣雨  降雨    大雨   豪雨  東北季風   寒流    鋒面   低氣壓  熱帶性低氣壓  華南雲雨區  \\\n",
      "0  2012   505   148   0    79    7   323    3   282   160      76      4   \n",
      "1  2013   692   160   1    99   16   374   17   304   190     114     19   \n",
      "2  2014   963   178   0    80   16    58   26   372   189      90     16   \n",
      "3  2015   788    91   0    61   10    44   16   245    80      45     12   \n",
      "4  2016  1168   182   1   147   23    79   25   352   200     117     24   \n",
      "5  2017  1007   179  10   127   44    95    7   361   243     155     24   \n",
      "6  2018  1086   188   8   190   55   120   27   281   261     185     14   \n",
      "7  2019  1120   149  17   204   59   160    0   318   171      77     24   \n",
      "8  2020   913   218  12   269   59   136   21   297   148      65     21   \n",
      "9   sum  8242  1493  49  1256  289  1389  142  2812  1642     924    158   \n",
      "\n",
      "   西南氣流  溫帶氣旋  強陣風  \n",
      "0    10     6   47  \n",
      "1     0     6   76  \n",
      "2     9     9   88  \n",
      "3    13     1   52  \n",
      "4     6     5   91  \n",
      "5    32     4  126  \n",
      "6    32     9  126  \n",
      "7     2    10  189  \n",
      "8     4     7  198  \n",
      "9   108    57  993  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "targets = ['雷雨', '雷陣雨', '降雨', '大雨', '豪雨', '東北季風', '寒流', \n",
    "           '鋒面', '低氣壓','熱帶性低氣壓', '華南雲雨區', '西南氣流', '溫帶氣旋', '強陣風']\n",
    "\n",
    "wc_selected = { k: wc[k] for k in targets }\n",
    "wc_selected['year'] = 'sum'\n",
    "\n",
    "data = []\n",
    "for year in range(2012, 2021):\n",
    "    print(year)\n",
    "    tmp = count_words('D:/data/cwb_report/'+str(year), min_count=1, min_length=2)\n",
    "    tmp_selected = {'year': year}\n",
    "    for k in targets:\n",
    "        try:\n",
    "            tmp_selected[k] = tmp[k]\n",
    "        except KeyError:\n",
    "            #print('No '+k)\n",
    "            tmp_selected[k] = 0\n",
    "    #print(tmp_selected)\n",
    "    data.append(tmp_selected)\n",
    "    \n",
    "data.append(wc_selected)\n",
    "data = pd.DataFrame(data)\n",
    "print(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../data/cwb_report_keyword_by_year.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
